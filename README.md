# <center>**Industrial Placement Project Work and Outputs**</center> 

<img src="images/cover_page/video_surveillance_cartoon.jpg" width=100% height=75%>

> **IMPORTANT**: mind that most of work I have done during my placement has been specified containing sensitive contents, therefore, after negotiation with the industry tutor, it has been agreed that, project works cannot be shown directly, but, I am allowed to use the same technique learnt from the placement and produce something else that is not straightly related to the interest of the company with providing very little portion of the source code. 

</br>

<h1 align="center">
	Data Augmentation
</h1>

As being working for the video surveillance sub-division of the media engineering department I dedicated my intern in helping developing AI systems that track, recognise the specified identities of faces, cars and license plates. Accompanying by the universalised 5G network, Huawei aims to build a world where “everything connected” and will make enormous contributions to public security.

In order to let the AI rapidly learn and pursue identity recognition, it is common knowledge that the more data an machine learning algorithm has access to, the more effective it can be. However in the case when the data is of lower quality despite the fact that the outcome will still be better, algorithms extracting useful data is necessary which will cost computer and lower the yield efficiency.

However, on the other hand, the project is to some extent hovering on the brink of touching the public privacy grey area due that the raw data collection involves camera-shooting captures of ordinary citizens’ faces under various circumstances and match it with the limited ID pictures police has provided. The constraints in raw data acquisition have a huge negative impact in the convergence of learning of AI thus data augmentation, which is the part I am taking charge of, where varying the key information while reserving the ID of datasets will play an important role in widening the diversity of the dataset. 

It is common knowledge that the more data an machine learning algorithm has access to, the more effective it can be. However in the case when the data is of lower quality, despite the fact that the outcome will still be better, algorithms extracting useful data is necessary which will cost computer and lower the yield efficiency. This inspires the idea that rather than starting with an extremely large corpus of unstructured, unfiltered and unlabelled data (e.g. copied directly from webpages) that contains many errors, it would be more effective if we take a small, curated corpus of structured data and augment in a way that increases the performance of models trained on it. The procedure can be specified as following steps below:

- [Video Data Acquisition](./images/videoData_acquisition/README.md)
- [2D Image Processing Approaches](./images/2Dprocessing/README.md)
- [3D Construction Aprroaches](./images/3Dprocessing/README.md)
- [Mesa Package & Software Rendering](./images/mesa/README.md)
- boost python 
- [Reflection and Development](./images/learning_schedule/README.md)
- transformers





<br/>